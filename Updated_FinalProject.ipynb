{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FinalProject2025.pdf**\n",
    "\n",
    "***2.2 Neuronal functional connectivity analysis***\n",
    "\n",
    "Data: The dataset include 130 simultaneously recorded neuros from the gustatory cortex while the rat learn to associate a novel taste with malaise. The data\n",
    "is located in the Moodle page (titled \"Anan3\"). See more instractions there.\n",
    "\n",
    "Main goal: Estimate the connection strength between the neurons to create\n",
    "a functional connectivity map.\n",
    "\n",
    "Tasks:\n",
    "1. First define the connectivity matrix between neurons. Use some statistic\n",
    "to test for significant connectivity\n",
    "2. Compute a measure of the strength of the connection.\n",
    "3. Use some methods to visualize the connectivity other than a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**instruction from moodle page under \"Anan3\"**\n",
    "\n",
    "***Assessing  neuronal  network functional connectivity from spike trains***\n",
    "\n",
    "**Data**: spiking activity of ensemble of ~130 neurons in the gustatory and somatosensory cortices\n",
    "recorded continuously (across ~30 hours) as rats learn to associate a novel taste with gastric malaise.\n",
    "\n",
    "**Your task**: find the connectivity between pairs of neurons in the network to show the changes across\n",
    "time, and learning. (Hint: cross correlation…)\n",
    "\n",
    "**Bonus**: use some cool packages for network representation to show the network topology. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input files:\n",
    "\n",
    "cluster_info.tsv – a Tab-separated values file which contains information about the sorted neurons.\n",
    "Each line contains the information for one optional neuron (sorted cluster). For your analysis only\n",
    "take neurons (clusters) that their “group” value is “good” (should be around 130).\n",
    "\n",
    "spike_seconds.npy – a list of spike times (in seconds)\n",
    "\n",
    "spike_clusters.npy – a list of cluster ids, which correspond to the spikes in spike_seconds.npy\n",
    "\n",
    "so if you load both files:\n",
    "\n",
    "cluster_ids = np.load(‘spike_clusters.npy’)\n",
    "\n",
    "st = np.load('spike_seconds.npy')\n",
    "\n",
    "if cluster_ids[0] = 5 and st[0] = 11.56, it means that there was a spike for cluster 5 at 11.56 seconds\n",
    "from the beginning of the recording."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Filter Data\n",
    "We first load the spike time data and filter it to include only 'good' neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spike_data(spike_times_file, spike_clusters_file, cluster_info_file):\n",
    "    \"\"\"Load and filter spike data for 'good' neurons.\"\"\"\n",
    "    spike_times = np.load(spike_times_file)  # Load spike times (in seconds)\n",
    "    spike_clusters = np.load(spike_clusters_file)  # Load neuron IDs\n",
    "\n",
    "    # Load cluster info and filter for 'good' neurons\n",
    "    cluster_info = pd.read_csv(cluster_info_file, sep='\\t')\n",
    "    good_neurons = cluster_info[cluster_info['group'] == 'good']['cluster_id'].values\n",
    "\n",
    "    # Filter spikes for 'good' neurons\n",
    "    mask = np.isin(spike_clusters, good_neurons)\n",
    "    filtered_spike_clusters = spike_clusters[mask]\n",
    "    filtered_spike_times = spike_times[mask]\n",
    "\n",
    "    return filtered_spike_times, filtered_spike_clusters, good_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good neurons: 131\n",
      "\n",
      "First few spike times: [[0.00833343]\n",
      " [0.00853344]\n",
      " [0.00916678]\n",
      " [0.01016679]\n",
      " [0.01626686]\n",
      " [0.02190027]\n",
      " [0.02316695]\n",
      " [0.03030037]\n",
      " [0.03130038]\n",
      " [0.03410042]]\n",
      "\n",
      "First few spike clusters: [154 923 917 845 112  70 251 954 845 921]\n"
     ]
    }
   ],
   "source": [
    "# sample output\n",
    "\n",
    "spike_times, spike_clusters, good_neurons = load_spike_data(\n",
    "    \"spike_seconds.npy\", \"spike_clusters.npy\", \"cluster_info.tsv\"\n",
    ")\n",
    "print(\"Number of good neurons:\", len(good_neurons))\n",
    "print(\"\\nFirst few spike times:\", spike_times[:10])\n",
    "print(\"\\nFirst few spike clusters:\", spike_clusters[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20530010 20530010\n"
     ]
    }
   ],
   "source": [
    "print(len(spike_times), len(spike_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spike_times shape: (20530010, 1)\n",
      "spike_clusters shape: (20530010,)\n"
     ]
    }
   ],
   "source": [
    "print(\"spike_times shape:\", spike_times.shape)\n",
    "print(\"spike_clusters shape:\", spike_clusters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_times = spike_times.flatten()  # Convert (N,1) → (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_segments(spike_times, segment_duration):\n",
    "    \"\"\"Splits spike times into fixed-size segments.\"\"\"\n",
    "    total_duration = np.max(spike_times)\n",
    "    # num_segments = int(np.ceil(total_duration / segment_duration))\n",
    "    num_segments = int(np.max(spike_times) // segment_duration)  # Only full segments\n",
    "    \n",
    "    segment_ranges = [\n",
    "        # (i * segment_duration, min((i + 1) * segment_duration, total_duration))\n",
    "        (i * segment_duration, (i + 1) * segment_duration) \n",
    "        for i in range(num_segments)\n",
    "    ]\n",
    "    \n",
    "    return segment_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_spike_trains(spike_times, spike_clusters, neuron_ids, segment_range, bin_size=0.001, binary_mode=False):\n",
    "    \"\"\"\n",
    "    Converts spike timestamps into a fixed-size binned spike train matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - binary_mode (bool): If True, store 0/1 (binary); otherwise, count spikes.\n",
    "\n",
    "    Returns:\n",
    "    - binned_spike_train (np.array): Fixed-size binned spike matrix (neurons × time bins).\n",
    "    \"\"\"\n",
    "    start_time, end_time = segment_range\n",
    "    num_bins = int((end_time - start_time) / bin_size)  \n",
    "\n",
    "    # Select spikes within this segment\n",
    "    mask = (spike_times >= start_time) & (spike_times < end_time)\n",
    "    segment_spike_times = spike_times[mask] - start_time  \n",
    "    segment_spike_clusters = spike_clusters[mask]\n",
    "\n",
    "    # Initialize spike train matrix (ensuring fixed shape)\n",
    "    binned_spike_train = np.zeros((len(neuron_ids), num_bins), dtype=np.uint8)\n",
    "\n",
    "    # Convert spike times to bin indices\n",
    "    bin_indices = (segment_spike_times / bin_size).astype(int)\n",
    "\n",
    "    # Map neuron ID to row index for fast lookup\n",
    "    neuron_idx_map = {neuron: i for i, neuron in enumerate(neuron_ids)}\n",
    "\n",
    "    # Populate the matrix\n",
    "    for spike_idx, neuron_id in enumerate(segment_spike_clusters):\n",
    "        if neuron_id in neuron_idx_map:\n",
    "            row = neuron_idx_map[neuron_id]\n",
    "            col = bin_indices[spike_idx], num_bins - 1\n",
    "            if binary_mode:\n",
    "                binned_spike_train[row, col] = 1  # Store 0/1 only\n",
    "            else:\n",
    "                binned_spike_train[row, col] += 1  # Store spike count\n",
    "\n",
    "    return binned_spike_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing & Saving Segment 1/15: 0s to 7200s\n",
      "✅ Segment 1 saved: binned_segments/binned_segment_0.npz\n",
      "Processing & Saving Segment 2/15: 7200s to 14400s\n",
      "✅ Segment 2 saved: binned_segments/binned_segment_1.npz\n",
      "Processing & Saving Segment 3/15: 14400s to 21600s\n",
      "✅ Segment 3 saved: binned_segments/binned_segment_2.npz\n",
      "Processing & Saving Segment 4/15: 21600s to 28800s\n",
      "✅ Segment 4 saved: binned_segments/binned_segment_3.npz\n",
      "Processing & Saving Segment 5/15: 28800s to 36000s\n",
      "✅ Segment 5 saved: binned_segments/binned_segment_4.npz\n",
      "Processing & Saving Segment 6/15: 36000s to 43200s\n",
      "✅ Segment 6 saved: binned_segments/binned_segment_5.npz\n",
      "Processing & Saving Segment 7/15: 43200s to 50400s\n",
      "✅ Segment 7 saved: binned_segments/binned_segment_6.npz\n",
      "Processing & Saving Segment 8/15: 50400s to 57600s\n",
      "✅ Segment 8 saved: binned_segments/binned_segment_7.npz\n",
      "Processing & Saving Segment 9/15: 57600s to 64800s\n",
      "✅ Segment 9 saved: binned_segments/binned_segment_8.npz\n",
      "Processing & Saving Segment 10/15: 64800s to 72000s\n",
      "✅ Segment 10 saved: binned_segments/binned_segment_9.npz\n",
      "Processing & Saving Segment 11/15: 72000s to 79200s\n",
      "✅ Segment 11 saved: binned_segments/binned_segment_10.npz\n",
      "Processing & Saving Segment 12/15: 79200s to 86400s\n",
      "✅ Segment 12 saved: binned_segments/binned_segment_11.npz\n",
      "Processing & Saving Segment 13/15: 86400s to 93600s\n",
      "✅ Segment 13 saved: binned_segments/binned_segment_12.npz\n",
      "Processing & Saving Segment 14/15: 93600s to 100800s\n",
      "✅ Segment 14 saved: binned_segments/binned_segment_13.npz\n",
      "Processing & Saving Segment 15/15: 100800s to 108000s\n",
      "✅ Segment 15 saved: binned_segments/binned_segment_14.npz\n",
      "✅ All segments processed and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "def process_and_save_segments(spike_times, spike_clusters, neuron_ids, bin_size, segment_duration):\n",
    "    \"\"\"\n",
    "    Processes and saves each fixed-size segment to disk to avoid memory issues.\n",
    "\n",
    "    Parameters:\n",
    "    - spike_times (np.array): Array of spike times in seconds.\n",
    "    - spike_clusters (np.array): Corresponding neuron IDs for each spike.\n",
    "    - neuron_ids (np.array): Unique neuron identifiers (good neurons).\n",
    "    - bin_size (float): Time bin size in seconds.\n",
    "    - segment_duration (int): Duration of each segment in seconds.\n",
    "\n",
    "    Saves:\n",
    "    - Each segment's binned spike train as a `.npz` file.\n",
    "    \"\"\"\n",
    "    output_dir = \"binned_segments\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    segment_ranges = split_into_segments(spike_times, segment_duration)\n",
    "    \n",
    "    for idx, segment_range in enumerate(segment_ranges):\n",
    "        print(f\"Processing & Saving Segment {idx + 1}/{len(segment_ranges)}: {segment_range[0]}s to {segment_range[1]}s\")\n",
    "\n",
    "        # Bin spike trains for the segment\n",
    "        binned_data = bin_spike_trains(spike_times, spike_clusters, neuron_ids, segment_range, bin_size)\n",
    "\n",
    "        # Save to compressed .npz file\n",
    "        np.savez_compressed(\n",
    "            os.path.join(output_dir, f\"binned_segment_{idx}.npz\"),\n",
    "            spike_train=binned_data,\n",
    "            neuron_ids=neuron_ids\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Segment {idx + 1} saved: {output_dir}/binned_segment_{idx}.npz\")\n",
    "\n",
    "# Run processing with memory-efficient saving\n",
    "bin_size = 0.001  # 1 ms = 1000 Hz binning\n",
    "segment_duration = 7200  # 2 hours = 7200 seconds\n",
    "\n",
    "process_and_save_segments(spike_times, spike_clusters, good_neurons, bin_size, segment_duration)\n",
    "\n",
    "print(\"✅ All segments processed and saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import correlate\n",
    "\n",
    "def compute_cross_correlation(binned_spike_train, max_lag=40):\n",
    "    \"\"\"\n",
    "    Computes the cross-correlation function for all neuron pairs.\n",
    "\n",
    "    Parameters:\n",
    "    - binned_spike_train (np.array): Shape (num_neurons, num_bins), 0/1 values.\n",
    "    - max_lag (int): Max time lag in milliseconds.\n",
    "\n",
    "    Returns:\n",
    "    - cross_corr_matrices (dict): Dictionary where keys are neuron pairs (i, j),\n",
    "      values are the cross-correlation functions.\n",
    "    \"\"\"\n",
    "    num_neurons = binned_spike_train.shape[0]\n",
    "    cross_corr_matrices = {}\n",
    "\n",
    "    for i in range(num_neurons):\n",
    "        for j in range(num_neurons):\n",
    "            if i != j:  # Skip self-correlation\n",
    "                corr = correlate(binned_spike_train[i], binned_spike_train[j], mode='full')\n",
    "                lag_center = len(corr) // 2\n",
    "                cross_corr_matrices[(i, j)] = corr[lag_center - max_lag : lag_center + max_lag]  # ±max_lag region\n",
    "\n",
    "    return cross_corr_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "# Load a segment\n",
    "segment_idx = 0\n",
    "file_path = f\"binned_segments/binned_segment_{segment_idx}.npz\"\n",
    "data = np.load(file_path)\n",
    "binned_spike_train = data[\"spike_train\"]\n",
    "\n",
    "# Compute cross-correlation\n",
    "cross_corr_matrices = compute_cross_correlation(binned_spike_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
